{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes for Machine Learning\n",
    "\n",
    "> [pdf](./../RW-2006-gaussian-processes-for-machine-learning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gaussian_process.GPfunctions as gp\n",
    "from gaussian_process import GaussianProcess\n",
    "from gaussian_process.kernels import SquaredExponentialKernel, CubicKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "\n",
    "objectiveFunction = lambda x: -x * np.sin(x)\n",
    "objectiveFunctionDerivative = lambda x: -x * np.cos(x) - np.sin(x)\n",
    "\n",
    "X = np.linspace(start=-3.0, stop=3.0, num=1_000)\n",
    "y = objectiveFunction(X)\n",
    "g = objectiveFunctionDerivative(X)\n",
    "\n",
    "sample_count = 6\n",
    "rng = np.random.default_rng(1)\n",
    "training_indices = rng.choice(np.arange(y.size), size=sample_count, replace=False)\n",
    "X_train, y_train, g_train = (\n",
    "    X[training_indices],\n",
    "    y[training_indices],\n",
    "    g[training_indices],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = SquaredExponentialKernel()\n",
    "\n",
    "GP_prior = GaussianProcess(kernel)\n",
    "\n",
    "prior_mean, prior_variance = GP_prior(X)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "gp.plot_gp(ax1, X, prior_mean, prior_variance)\n",
    "gp.plot_samples(ax1, X, [GP_prior.sample(rng, X) for _ in range(3)])\n",
    "gp.plot_label(ax1, \"Prior\")\n",
    "\n",
    "GP_posterior = GaussianProcess(kernel, x_known=X_train, f_known=y_train, f_noise=1e-14)\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "\n",
    "gp.plot_objective(ax2, X, y, X_train, y_train)\n",
    "gp.plot_gp(ax2, X, posterior_mean, posterior_variance)\n",
    "gp.plot_samples(ax2, X, [GP_posterior.sample(rng, X) for _ in range(3)])\n",
    "gp.plot_label(ax2, \"Posterior\")\n",
    "\n",
    "fig.suptitle(\"Squared exponential kernel sampled\")\n",
    "fig.set_figwidth(15)\n",
    "plt.show()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "gp.plot_gp(ax1, X, prior_mean, prior_variance)\n",
    "gp.plot_label(ax1, \"Prior\")\n",
    "\n",
    "gp.plot_objective(ax2, X, y, X_train, y_train)\n",
    "gp.plot_gp(ax2, X, posterior_mean, posterior_variance)\n",
    "gp.plot_label(ax2, \"Posterior\")\n",
    "\n",
    "fig.suptitle(\"Squared exponential kernel\")\n",
    "fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cubic spline kernel\n",
    "# Sampling does not work, since covariance matrix is not symmetric positive-definite\n",
    "\n",
    "kernel = CubicKernel()\n",
    "\n",
    "GP_prior = GaussianProcess(kernel)\n",
    "GP_posterior = GaussianProcess(kernel, x_known=X_train, f_known=y_train)\n",
    "\n",
    "prior_mean, prior_variance = GP_prior(X)\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "gp.plot_gp(ax1, X, prior_mean, prior_variance)\n",
    "gp.plot_label(ax1, \"Prior\")\n",
    "\n",
    "gp.plot_objective(ax2, X, y, X_train, y_train)\n",
    "gp.plot_gp(ax2, X, posterior_mean, posterior_variance)\n",
    "gp.plot_label(ax2, \"Posterior\")\n",
    "\n",
    "fig.suptitle(\"Cubic kernel\")\n",
    "fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP with derivative information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = SquaredExponentialKernel()\n",
    "\n",
    "GP_posterior = GaussianProcess(\n",
    "    kernel=kernel,\n",
    "    x_known=X_train,\n",
    "    f_known=y_train,\n",
    "    g_known=g_train,\n",
    "    f_noise=1e-14,\n",
    "    g_noise=1e-14,\n",
    ")\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, sharey=True)\n",
    "\n",
    "gp.plot_objective(ax1, X, y, X_train, y_train)\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_variance)\n",
    "gp.plot_label(ax1, \"Posterior\")\n",
    "\n",
    "fig.suptitle(\"Squared exponential kernel with gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = CubicKernel()\n",
    "\n",
    "GP_posterior = GaussianProcess(\n",
    "    kernel=kernel, x_known=X_train, f_known=y_train, g_known=g_train\n",
    ")\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, sharey=True)\n",
    "\n",
    "gp.plot_objective(ax1, X, y, X_train, y_train)\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_variance)\n",
    "gp.plot_label(ax1, \"Posterior\")\n",
    "\n",
    "fig.suptitle(\"Cubic kernel with gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative vs no derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = CubicKernel()\n",
    "\n",
    "GP_posterior = GaussianProcess(kernel=kernel, x_known=X_train, f_known=y_train)\n",
    "GP_posterior_gradients = GaussianProcess(\n",
    "    kernel=kernel, x_known=X_train, f_known=y_train, g_known=g_train\n",
    ")\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "\n",
    "gp.plot_objective(ax1, X, y, X_train, y_train)\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_variance, label=\"No gradient\")\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior_gradients(X)\n",
    "\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_variance, label=\"Gradient\")\n",
    "\n",
    "gp.plot_label(ax1, \"Cubic kernal\")\n",
    "fig.suptitle(\"Gradient vs no gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = SquaredExponentialKernel()\n",
    "\n",
    "GP_posterior = GaussianProcess(\n",
    "    kernel=kernel, x_known=X_train, f_known=y_train, f_noise=1e-14\n",
    ")\n",
    "GP_posterior_gradients = GaussianProcess(\n",
    "    kernel=kernel,\n",
    "    x_known=X_train,\n",
    "    f_known=y_train,\n",
    "    g_known=g_train,\n",
    "    f_noise=1e-14,\n",
    "    g_noise=1e-14,\n",
    ")\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "\n",
    "gp.plot_objective(ax1, X, y, X_train, y_train)\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_variance, label=\"No gradient\")\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior_gradients(X)\n",
    "\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_variance, label=\"Gradient\")\n",
    "\n",
    "gp.plot_label(ax1, \"Squared exponential kernel\")\n",
    "fig.suptitle(\"Gradient vs no gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error at points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_posterior = GaussianProcess(\n",
    "    SquaredExponentialKernel(), X_train, y_train, f_noise=1e-14\n",
    ")\n",
    "print(\"For gp with squared exponential kernel and no gradient info we have:\")\n",
    "for x, f in zip(X_train, y_train):\n",
    "    print(f\"x={x}: abs(f(x)-gp(x))={np.abs(f-GP_posterior(x)[0])}\")\n",
    "print(f\"With a total f-error of {GP_posterior.f_error()}\")\n",
    "\n",
    "GP_posterior = GaussianProcess(CubicKernel(), X_train, y_train)\n",
    "print()\n",
    "print(\"For gp with cubic kernel and no gradient info we have:\")\n",
    "for x, f in zip(X_train, y_train):\n",
    "    print(f\"x={x}: abs(f(x)-gp(x))={np.abs(f-GP_posterior(x)[0])}\")\n",
    "print(f\"With a total f-error of {GP_posterior.f_error()}\")\n",
    "\n",
    "GP_posterior = GaussianProcess(\n",
    "    SquaredExponentialKernel(), X_train, y_train, g_train, f_noise=1e-14, g_noise=1e-14\n",
    ")\n",
    "print()\n",
    "print(\"For gp with squared exponential kernel and gradient info we have:\")\n",
    "for x, f, g in zip(X_train, y_train, g_train):\n",
    "    print(\n",
    "        f\"x={x}: abs(f(x)-gp(x))={np.abs(f-GP_posterior(x)[0])}, abs(g(x)-g_gp(x))={np.abs(g-GP_posterior.derivative(x)[0])}\"\n",
    "    )\n",
    "print(f\"With a total f-error of {GP_posterior.f_error()}\")\n",
    "print(f\"With a total g-error of {GP_posterior.g_error()}\")\n",
    "\n",
    "GP_posterior = GaussianProcess(CubicKernel(), X_train, y_train, g_train)\n",
    "print()\n",
    "print(\"For gp with cubic kernel and gradient info we have:\")\n",
    "for x, f, g in zip(X_train, y_train, g_train):\n",
    "    print(\n",
    "        f\"x={x}: abs(f(x)-gp(x))={np.abs(f-GP_posterior(x)[0])}, abs(g(x)-g_gp(x))={np.abs(g-GP_posterior.derivative(x)[0])}\"\n",
    "    )\n",
    "print(f\"With a total f-error of {GP_posterior.f_error()}\")\n",
    "print(f\"With a total g-error of {GP_posterior.g_error()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sklearn = X.reshape(-1, 1)\n",
    "y_sklearn = y.reshape(-1, 1)\n",
    "\n",
    "X_train_sklearn = X_train.reshape(-1, 1)\n",
    "y_train_sklearn = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via sklearn\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "\n",
    "l = 1.0\n",
    "\n",
    "kernel = ConstantKernel(1.0, constant_value_bounds=\"fixed\") * RBF(\n",
    "    length_scale=l, length_scale_bounds=\"fixed\"\n",
    ")\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X_sklearn, return_std=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "gp.plot_gp(ax1, X_sklearn.ravel(), mean_prediction, std_prediction)\n",
    "gp.plot_samples(\n",
    "    ax1,\n",
    "    X_sklearn,\n",
    "    [gaussian_process.sample_y(X_sklearn, random_state=i) for i in range(3)],\n",
    ")\n",
    "gp.plot_label(ax1, \"Prior\")\n",
    "\n",
    "gaussian_process.fit(X_train_sklearn, y_train_sklearn)\n",
    "\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X_sklearn, return_std=True)\n",
    "\n",
    "gp.plot_objective(ax2, X_sklearn, y_sklearn, X_train_sklearn, y_train_sklearn)\n",
    "gp.plot_gp(ax2, X_sklearn.ravel(), mean_prediction, std_prediction)\n",
    "gp.plot_samples(\n",
    "    ax2,\n",
    "    X_sklearn,\n",
    "    [gaussian_process.sample_y(X_sklearn, random_state=i) for i in range(3)],\n",
    ")\n",
    "gp.plot_label(ax2, \"Posterior\")\n",
    "\n",
    "fig.suptitle(\"Sklearn\")\n",
    "fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare custom vs sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_custom = GaussianProcess(\n",
    "    SquaredExponentialKernel(l=l), X_train, y_train, f_noise=1e-10\n",
    ")  # sklearn adds 1e-10 noise\n",
    "\n",
    "mean, variance = GP_custom(X)\n",
    "std = GP_custom.std_deviation(X, variance)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "gp.plot_objective(ax1, X, y, X_train, y_train)\n",
    "gp.plot_gp(ax1, X, mean, std, label=\"custom\")\n",
    "gp.plot_gp(ax1, X_sklearn.ravel(), mean_prediction, std_prediction, label=\"sklearn\")\n",
    "gp.plot_label(ax1, \"Posterior\")\n",
    "\n",
    "fig.suptitle(\"Sklearn vs custom\")\n",
    "fig.set_figwidth(15)\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Difference in log marginal likelihood is {np.abs(GP_custom.log_marginal_likelihood() - gaussian_process.log_marginal_likelihood())}\"\n",
    ")\n",
    "print(f\"Max difference in mean is {np.abs(mean - mean_prediction).max()}\")\n",
    "print(f\"max difference in std is {np.abs(std - std_prediction).max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
