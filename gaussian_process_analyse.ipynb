{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a specific Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Std at training points is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gaussian_process.GPfunctions as gp\n",
    "from gaussian_process import GaussianProcess\n",
    "from gaussian_process.kernels import SquaredExponentialKernel, CubicKernel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(start=0.0, stop=1.0, num=1_000)\n",
    "\n",
    "# Solve by varying hyperparameters\n",
    "kernel = SquaredExponentialKernel(l=0.1)\n",
    "\n",
    "X_train = np.array([0.0, 1.0, 0.5, 0.21797318, 0.83333875])\n",
    "f_train = np.array(\n",
    "    [4.54568576e-07, 1.45570662e-07, 2.47127271e-07, 3.51116916e-07, 1.67656995e-07]\n",
    ")\n",
    "g_train = np.array(\n",
    "    [\n",
    "        -5.207673619287845e-07,\n",
    "        -9.722857176215605e-08,\n",
    "        -3.08997887937976e-07,\n",
    "        -4.2844720967864284e-07,\n",
    "        -1.678160337877302e-07,\n",
    "    ]\n",
    ")\n",
    "f_noise = 1e-14\n",
    "g_noise = 1e-14\n",
    "\n",
    "GP_posterior = GaussianProcess(\n",
    "    kernel,\n",
    "    x_known=X_train,\n",
    "    f_known=f_train,\n",
    "    g_known=g_train,\n",
    "    f_noise=f_noise,\n",
    "    g_noise=g_noise,\n",
    ")\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "posterior_std = GP_posterior.std_deviation(X, variance=posterior_variance)\n",
    "posterior_mean_derivative, posterior_mean_derivative_variance = GP_posterior.derivative(\n",
    "    X\n",
    ")\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, sharey=True)\n",
    "\n",
    "ax1.scatter(X_train, f_train, label=\"Observations\")\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_std)\n",
    "gp.plot_label(ax1, \"Gaussian Process\")\n",
    "\n",
    "fig.suptitle(\"Squared exponential kernel\")\n",
    "fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linalg error because of cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(\n",
    "    [\n",
    "        0.00000000e00,\n",
    "        1.00000000e00,\n",
    "        1.69918210e-01,\n",
    "        2.41870758e-05,\n",
    "        2.07987347e-05,\n",
    "        2.07963787e-05,\n",
    "        2.08048677e-05,\n",
    "    ]\n",
    ")\n",
    "f_train = np.array(\n",
    "    [\n",
    "        1.19884924e02,\n",
    "        1.59770254e10,\n",
    "        4.61198173e08,\n",
    "        1.13152862e02,\n",
    "        1.12969950e02,\n",
    "        1.12969951e02,\n",
    "        1.12969950e02,\n",
    "    ]\n",
    ")\n",
    "g_train = np.array(\n",
    "    [\n",
    "        -664786.5771969751,\n",
    "        31954716085.865658,\n",
    "        5429136006.379254,\n",
    "        108120.58844165073,\n",
    "        -155.13284739461844,\n",
    "        -230.42261970330514,\n",
    "        40.849419301530375,\n",
    "    ]\n",
    ")\n",
    "f_noise = 1e-14 * 10\n",
    "g_noise = 1e-14 * 10\n",
    "\n",
    "kernel = SquaredExponentialKernel(l=1.0 / (2 * len(X_train)))\n",
    "\n",
    "GP_posterior = GaussianProcess(\n",
    "    kernel,\n",
    "    x_known=X_train,\n",
    "    f_known=f_train,\n",
    "    g_known=g_train,\n",
    "    f_noise=f_noise,\n",
    "    g_noise=g_noise,\n",
    ")\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "posterior_std = GP_posterior.std_deviation(X, variance=posterior_variance)\n",
    "posterior_mean_derivative, posterior_mean_derivative_variance = GP_posterior.derivative(\n",
    "    X\n",
    ")\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, sharey=True)\n",
    "\n",
    "ax1.scatter(X_train, f_train, label=\"Observations\")\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_std)\n",
    "gp.plot_label(ax1, \"Gaussian Process\")\n",
    "\n",
    "fig.suptitle(\"Squared exponential kernel sampled\")\n",
    "fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weard mean behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    0.0,\n",
    "    1.0,\n",
    "    0.16986163,\n",
    "    0.1698619,\n",
    "    0.27932006,\n",
    "    0.07481004,\n",
    "    0.2447848,\n",
    "    0.27933006,\n",
    "    0.46242802,\n",
    "    0.46243802,\n",
    "    0.40427808,\n",
    "    0.47887303,\n",
    "    0.49980605,\n",
    "    0.68352651,\n",
    "    0.78379573,\n",
    "    0.87313681,\n",
    "    0.60982003,\n",
    "    0.93456452,\n",
    "    0.73265708,\n",
    "    0.56522082,\n",
    "    0.34525749,\n",
    "    0.82919883,\n",
    "    0.11455123,\n",
    "    0.64672027,\n",
    "    0.032554,\n",
    "    0.96846498,\n",
    "    0.37346661,\n",
    "    0.53635887,\n",
    "    0.75770305,\n",
    "]\n",
    "f_train = [\n",
    "    -2.64136323e03,\n",
    "    6.25833183e10,\n",
    "    -2.74307652e03,\n",
    "    -2.74307668e03,\n",
    "    -2.80969822e03,\n",
    "    -2.68590311e03,\n",
    "    -2.78859218e03,\n",
    "    -2.80970434e03,\n",
    "    -2.92223378e03,\n",
    "    -2.92223983e03,\n",
    "    -2.88657072e03,\n",
    "    -2.93205537e03,\n",
    "    1.00030725e10,\n",
    "    2.59145402e10,\n",
    "    3.66358993e10,\n",
    "    4.70110092e10,\n",
    "    1.88602276e10,\n",
    "    5.44609171e10,\n",
    "    3.10249786e10,\n",
    "    1.50052839e10,\n",
    "    -2.85018820e03,\n",
    "    4.18286472e10,\n",
    "    -2.70972835e03,\n",
    "    2.22926208e10,\n",
    "    -2.66069543e03,\n",
    "    5.86502450e10,\n",
    "    -2.86756992e03,\n",
    "    1.26974013e10,\n",
    "    3.37387824e10,\n",
    "]\n",
    "f_noise = 1e-14\n",
    "g_train = [\n",
    "    -5.92682961e02,\n",
    "    1.25166642e11,\n",
    "    -6.04898180e02,\n",
    "    -6.04898199e02,\n",
    "    -6.12226370e02,\n",
    "    -5.98073547e02,\n",
    "    -6.10035403e02,\n",
    "    -6.12226979e02,\n",
    "    -6.05077531e02,\n",
    "    -6.05071188e02,\n",
    "    -6.16557297e02,\n",
    "    -5.85363133e02,\n",
    "    7.00518444e10,\n",
    "    1.00921962e11,\n",
    "    1.12319346e11,\n",
    "    1.19484956e11,\n",
    "    9.01457975e10,\n",
    "    1.22870306e11,\n",
    "    1.06964050e11,\n",
    "    8.25923429e10,\n",
    "    -6.15687110e02,\n",
    "    1.16301348e11,\n",
    "    -6.00944515e02,\n",
    "    9.58021931e10,\n",
    "    -5.95020619e02,\n",
    "    1.24224591e11,\n",
    "    -6.16565038e02,\n",
    "    7.72766667e10,\n",
    "    1.09703959e11,\n",
    "]\n",
    "g_noise = 1e-14\n",
    "\n",
    "kernel = SquaredExponentialKernel(1.0 / (2 * len(X_train)))\n",
    "\n",
    "GP_posterior = GaussianProcess(\n",
    "    kernel,\n",
    "    x_known=X_train,\n",
    "    f_known=f_train,\n",
    "    g_known=g_train,\n",
    "    f_noise=f_noise,\n",
    "    g_noise=g_noise,\n",
    ")\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "posterior_std = GP_posterior.std_deviation(X, variance=posterior_variance)\n",
    "posterior_mean_derivative, posterior_mean_derivative_variance = GP_posterior.derivative(\n",
    "    X\n",
    ")\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, sharey=True)\n",
    "\n",
    "ax1.scatter(X_train, f_train, label=\"Observations\")\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_std)\n",
    "gp.plot_label(ax1, \"Gaussian Process\")\n",
    "\n",
    "fig.suptitle(\"Squared exponential kernel sampled\")\n",
    "fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrong gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From OSCIGRAD\n",
    "# When the function values and gradient contain values that are far apart, the GP will have large errors in the prediction.\n",
    "# Best thing to do seems to be to alter kernel hyperparameter. Most accurate kernel does not align with the one with max log marginal likelihood\n",
    "# Scaling does not help, since distance remains in same order and thus error decreases only in the absolute but not relative.\n",
    "\n",
    "X_train = [\n",
    "    0.00000000e00,\n",
    "    1.00000000e00,\n",
    "    2.01370962e-01,\n",
    "    2.85678888e-10,\n",
    "    3.12057312e-09,\n",
    "    3.12057212e-09,\n",
    "]\n",
    "f_train = [\n",
    "    4.88153517e-12,\n",
    "    1.41555445e08,\n",
    "    5.60060592e06,\n",
    "    9.24588434e-12,\n",
    "    1.28168841e-09,\n",
    "    1.28168757e-09,\n",
    "]\n",
    "g_train = [\n",
    "    -2.44151587e-02,\n",
    "    2.93727546e08,\n",
    "    5.55308816e07,\n",
    "    5.49693870e-02,\n",
    "    8.42730769e-01,\n",
    "    8.42730492e-01,\n",
    "]\n",
    "f_noise = 1e-10\n",
    "g_noise = 1e-10\n",
    "\n",
    "# Scaling\n",
    "# factor = 1e-8\n",
    "# f_train = [factor * f for f in f_train]\n",
    "# g_train = [factor * g for g in g_train]\n",
    "\n",
    "kernel = SquaredExponentialKernel(1.0 / (len(X_train) - 1))  # Average distance\n",
    "# kernel = SquaredExponentialKernel(min([np.abs(a-b) for a,b in itertools.pairwise(sorted(X_train))])) # Min distance\n",
    "# kernel = SquaredExponentialKernel(1e-5) # Max log-marginal-likelihood\n",
    "\n",
    "GP_posterior = GaussianProcess(\n",
    "    kernel,\n",
    "    x_known=X_train,\n",
    "    f_known=f_train,\n",
    "    g_known=g_train,\n",
    "    f_noise=f_noise,\n",
    "    g_noise=g_noise,\n",
    ")\n",
    "\n",
    "posterior_mean, posterior_variance = GP_posterior(X)\n",
    "posterior_std = GP_posterior.std_deviation(X, variance=posterior_variance)\n",
    "posterior_mean_derivative, posterior_mean_derivative_variance = GP_posterior.derivative(\n",
    "    X\n",
    ")\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, sharey=True)\n",
    "\n",
    "ax1.scatter(X_train, f_train, label=\"Observations\")\n",
    "gp.plot_gp(ax1, X, posterior_mean, posterior_std)\n",
    "gp.plot_label(ax1, \"Gaussian Process\")\n",
    "\n",
    "fig.suptitle(\"Squared exponential kernel sampled\")\n",
    "fig.set_figwidth(15)\n",
    "plt.show()\n",
    "\n",
    "print(\"For gp with squared exponential kernel and gradient info we have:\")\n",
    "for x, f, g in zip(X_train, f_train, g_train):\n",
    "    m, v = GP_posterior(x)\n",
    "    md, mdv = GP_posterior.derivative(x)\n",
    "    print(\n",
    "        f\"x={x}:\",\n",
    "        f\"  abs(f(x)-gp(x))={np.abs(f-m)} with f(x)={f} and gp(x)={m}\",\n",
    "        f\"  abs(g(x)-g_gp(x))={np.abs(g-md)} with g(x)={g} and g_gp(x)={md}\",\n",
    "        sep=os.linesep,\n",
    "    )\n",
    "print(f\"With total f-error of {GP_posterior.f_error()}\")\n",
    "print(f\"With total g-error of {GP_posterior.g_error()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean explosion between points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From VESUVIOULS\n",
    "X_train = [\n",
    "    0.0,\n",
    "    1.0,\n",
    "    0.34375,\n",
    "    0.68661499,\n",
    "    0.09500122,\n",
    "    0.8555491,\n",
    "    0.49509275,\n",
    "    0.20285714,\n",
    "    0.02356476,\n",
    "    0.59347234,\n",
    "    0.93369929,\n",
    "    0.27591779,\n",
    "    0.76596783,\n",
    "    0.42163831,\n",
    "    0.14440007,\n",
    "    0.54601187,\n",
    "    0.05377178,\n",
    "    0.80970868,\n",
    "    0.38147715,\n",
    "    0.64259053,\n",
    "    0.23888803,\n",
    "    0.96911578,\n",
    "    0.00559203,\n",
    "    0.45915459,\n",
    "    0.17208726,\n",
    "    0.72497403,\n",
    "    0.31003262,\n",
    "    0.89180041,\n",
    "    0.11728859,\n",
    "    0.52164628,\n",
    "    0.61812737,\n",
    "    0.07241361,\n",
    "]\n",
    "f_train = [\n",
    "    1.65107324,\n",
    "    8.99379119,\n",
    "    8.92435191,\n",
    "    8.96131554,\n",
    "    8.88700337,\n",
    "    8.97887865,\n",
    "    8.94105279,\n",
    "    8.90690344,\n",
    "    8.83277011,\n",
    "    8.95153002,\n",
    "    8.9869549,\n",
    "    8.91636456,\n",
    "    8.9695878,\n",
    "    8.93307233,\n",
    "    8.89779714,\n",
    "    8.94649865,\n",
    "    8.86995595,\n",
    "    8.97412959,\n",
    "    8.92861917,\n",
    "    8.95670267,\n",
    "    8.91172988,\n",
    "    8.99060821,\n",
    "    8.62604322,\n",
    "    8.93717023,\n",
    "    8.90238374,\n",
    "    8.96532029,\n",
    "    8.92044406,\n",
    "    8.98262777,\n",
    "    8.89248016,\n",
    "    8.94389979,\n",
    "    8.95413029,\n",
    "    8.87944161,\n",
    "]\n",
    "g_train = [\n",
    "    -7.51691324e04,\n",
    "    1.03022687e-01,\n",
    "    1.14435808e-01,\n",
    "    1.04565552e-01,\n",
    "    2.79635983e-01,\n",
    "    1.03494550e-01,\n",
    "    1.07538492e-01,\n",
    "    1.39864569e-01,\n",
    "    2.40176522e00,\n",
    "    1.05636836e-01,\n",
    "    1.03205924e-01,\n",
    "    1.21893985e-01,\n",
    "    1.03963687e-01,\n",
    "    1.09950301e-01,\n",
    "    1.78158532e-01,\n",
    "    1.06420841e-01,\n",
    "    6.44217345e-01,\n",
    "    1.03712337e-01,\n",
    "    1.11914488e-01,\n",
    "    1.05010020e-01,\n",
    "    1.28939287e-01,\n",
    "    1.03102095e-01,\n",
    "    7.03432482e01,\n",
    "    1.08570218e-01,\n",
    "    1.55172026e-01,\n",
    "    1.04247421e-01,\n",
    "    1.17526056e-01,\n",
    "    1.03349291e-01,\n",
    "    2.18209466e-01,\n",
    "    1.06913291e-01,\n",
    "    1.05302362e-01,\n",
    "    4.06580723e-01,\n",
    "]\n",
    "f_noise = 1e-10\n",
    "g_noise = 1e-10\n",
    "\n",
    "for c in [1e5, 1e3, 1e1, 1.0, 0.5, 1e-1, 1e-2]:\n",
    "    kernel = SquaredExponentialKernel(c / (len(X_train) - 1))  # c * Average distance\n",
    "\n",
    "    GP_posterior = GaussianProcess(\n",
    "        kernel,\n",
    "        x_known=X_train,\n",
    "        f_known=f_train,\n",
    "        g_known=g_train,\n",
    "        f_noise=f_noise,\n",
    "        g_noise=g_noise,\n",
    "    )\n",
    "\n",
    "    posterior_mean, posterior_variance = GP_posterior(X)\n",
    "    posterior_std = GP_posterior.std_deviation(X, variance=posterior_variance)\n",
    "    posterior_mean_derivative, posterior_mean_derivative_variance = (\n",
    "        GP_posterior.derivative(X)\n",
    "    )\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, sharey=True)\n",
    "\n",
    "    ax1.scatter(X_train, f_train, label=\"Observations\")\n",
    "    gp.plot_gp(ax1, X, posterior_mean, posterior_std)\n",
    "    gp.plot_label(ax1, \"Gaussian Process\")\n",
    "\n",
    "    fig.suptitle(f\"Squared exponential kernel c={c}\")\n",
    "    fig.set_figwidth(15)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
