{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-discrete Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gaussian_process.GPfunctions as gp\n",
    "from gaussian_process import GaussianProcess\n",
    "from gaussian_process.kernels import CubicKernel, SquaredExponentialKernel\n",
    "from acquisition import (\n",
    "    LowerConfidenceBoundVariance,\n",
    "    GP_LCB_Variance,\n",
    ")\n",
    "from acquisition.optimization import (\n",
    "    ScipyAcquisitionOptimizer,\n",
    "    GradientBinarySearchAcquisitionOptimizer,\n",
    "    DIRECTAcquisitionOptimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "\n",
    "objectiveFunction = lambda x: -x * np.sin(x)\n",
    "objectiveFunctionDerivative = lambda x: -x * np.cos(x) - np.sin(x)\n",
    "\n",
    "X = np.linspace(start=-3.0, stop=3.0, num=1_000)\n",
    "y = objectiveFunction(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "acquisitionOptimizer = DIRECTAcquisitionOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = np.array([-3.0, 3.0])\n",
    "y_sample = objectiveFunction(X_sample)\n",
    "\n",
    "x_best = X_sample[np.argmin(y_sample)]\n",
    "y_best = np.min(y_sample)\n",
    "\n",
    "for t in range(max_iter):\n",
    "    kernel = SquaredExponentialKernel(l=np.abs(X[0] - X[-1]) / (len(X_sample) - 1))\n",
    "    GP_posterior = GaussianProcess(kernel=kernel, x_known=X_sample, f_known=y_sample)\n",
    "\n",
    "    mean, variance = GP_posterior(X)\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, sharey=True)\n",
    "    gp.plot_objective(ax1, X, y, X_sample, y_sample)\n",
    "    gp.plot_gp(ax1, X, mean, variance)\n",
    "\n",
    "    acquisitionFunction = GP_LCB_Variance(GP_posterior, t, 1, nu=10)\n",
    "\n",
    "    ax1.plot(X, [acquisitionFunction(x) for x in X], label=\"Acquisition\")\n",
    "    ax1.plot(\n",
    "        X, [acquisitionFunction.derivative(x) for x in X], label=\"Acquisition Gradient\"\n",
    "    )\n",
    "\n",
    "    gp.plot_label(ax1, f\"{len(X_sample)} data points\")\n",
    "    plt.show()\n",
    "\n",
    "    maxAquisition = acquisitionOptimizer.maximize(\n",
    "        acquisitionFunction, -3.0, 3.0, X_sample\n",
    "    )\n",
    "\n",
    "    x_new = maxAquisition\n",
    "    y_new = objectiveFunction(x_new)\n",
    "    if x_new in X_sample:\n",
    "        print(\"Terminated due to insufficient acquisition function\")\n",
    "    else:\n",
    "        X_sample = np.append(X_sample, x_new)\n",
    "        y_sample = np.append(y_sample, y_new)\n",
    "        if y_new < y_best:\n",
    "            x_best, y_best = x_new, y_new\n",
    "    print(f\"best: x={x_best}, y={y_best}\")\n",
    "\n",
    "print(f\"best: ({x_best},{y_best})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization with gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = np.array([-3.0, 3.0])\n",
    "y_sample = objectiveFunction(X_sample)\n",
    "g_sample = objectiveFunctionDerivative(X_sample)\n",
    "\n",
    "x_best = X_sample[np.argmin(y_sample)]\n",
    "y_best = np.min(y_sample)\n",
    "\n",
    "for t in range(max_iter):\n",
    "    kernel = SquaredExponentialKernel(\n",
    "        l=np.abs(X[0] - X[-1]) / (2 * (len(X_sample) - 1))\n",
    "    )\n",
    "    GP_posterior = GaussianProcess(\n",
    "        kernel=kernel, x_known=X_sample, f_known=y_sample, g_known=g_sample\n",
    "    )\n",
    "\n",
    "    mean, variance = GP_posterior(X)\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, sharey=True)\n",
    "    gp.plot_objective(ax1, X, y, X_sample, y_sample)\n",
    "    gp.plot_gp(ax1, X, mean, variance)\n",
    "\n",
    "    acquisitionFunction = GP_LCB_Variance(GP_posterior, t, 1, nu=10)\n",
    "\n",
    "    ax1.plot(X, [acquisitionFunction(x) for x in X], label=\"Acquisition\")\n",
    "    ax1.plot(\n",
    "        X, [acquisitionFunction.derivative(x) for x in X], label=\"Acquisition Gradient\"\n",
    "    )\n",
    "\n",
    "    gp.plot_label(ax1, f\"{len(X_sample)} data points\")\n",
    "    plt.show()\n",
    "\n",
    "    maxAquisition = acquisitionOptimizer.maximize(\n",
    "        acquisitionFunction, -3.0, 3.0, X_sample\n",
    "    )\n",
    "\n",
    "    x_new = maxAquisition\n",
    "    y_new = objectiveFunction(x_new)\n",
    "    g_new = objectiveFunctionDerivative(x_new)\n",
    "    if x_new in X_sample:\n",
    "        print(\"Terminated due to insufficient acquisition function\")\n",
    "    else:\n",
    "        X_sample = np.append(X_sample, x_new)\n",
    "        y_sample = np.append(y_sample, y_new)\n",
    "        g_sample = np.append(g_sample, g_new)\n",
    "        if y_new < y_best:\n",
    "            x_best, y_best = x_new, y_new\n",
    "    print(f\"best: x={x_best}, y={y_best}\")\n",
    "\n",
    "print(f\"best: ({x_best},{y_best})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
